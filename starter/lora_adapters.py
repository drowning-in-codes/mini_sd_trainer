# æœ‰å‡ ç§è®­ç»ƒæŠ€å·§å¯ä»¥ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹ä»¥ç”Ÿæˆç‰¹å®šä¸»é¢˜æˆ–ç‰¹å®šé£æ ¼çš„å›¾åƒã€‚
# è¿™äº›è®­ç»ƒæ–¹æ³•ä¸­çš„æ¯ä¸€ç§éƒ½äº§ç”Ÿä¸åŒç±»å‹çš„é€‚é…å™¨ã€‚ä¸€äº›é€‚é…å™¨ç”Ÿæˆä¸€ä¸ªå…¨æ–°çš„æ¨¡å‹ï¼Œè€Œå…¶ä»–é€‚é…å™¨ä»…ä¿®æ”¹è¾ƒå°çš„åµŒå…¥æˆ–æƒé‡é›†ã€‚è¿™æ„å‘³ç€æ¯ä¸ªé€‚é…å™¨çš„åŠ è½½è¿‡ç¨‹ä¹Ÿä¸åŒ

# DreamBooth
# DreamBooth é€šè¿‡ä»…å¯¹ä¸»é¢˜çš„å‡ å¼ å›¾ç‰‡è¿›è¡Œå¾®è°ƒï¼Œæ¥ç”Ÿæˆè¯¥ä¸»é¢˜çš„æ–°é£æ ¼å’Œåœºæ™¯çš„å›¾åƒã€‚
# è¿™ç§æ–¹æ³•é€šè¿‡åœ¨æç¤ºè¯ä¸­ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„è¯ï¼Œè®©æ¨¡å‹å­¦ä¼šå°†å…¶ä¸ä¸»é¢˜å›¾åƒå…³è”èµ·æ¥ã€‚åœ¨æ‰€æœ‰è®­ç»ƒæ–¹æ³•ä¸­ï¼ŒDreamBooth äº§ç”Ÿçš„æ–‡ä»¶å¤§å°æœ€å¤§ï¼ˆé€šå¸¸å‡  GBï¼‰ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå®Œæ•´çš„æ£€æŸ¥ç‚¹æ¨¡å‹ã€‚
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained(
    "sd-dreambooth-library/herge-styled", torch_dtype=torch.float16
).to("cuda")


prompt = "A detailed description of the desired image"
image = pipeline(prompt).images[0]


from diffusers import AutoPipelineForText2Image

# textual inversion
# æ–‡æœ¬åè½¬ä¸ DreamBooth éå¸¸ç›¸ä¼¼ï¼Œå®ƒä¹Ÿå¯ä»¥é€šè¿‡å°‘é‡å›¾åƒæ¥ä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹ä»¥ç”Ÿæˆç‰¹å®šæ¦‚å¿µï¼ˆé£æ ¼ã€ç‰©ä½“ï¼‰ã€‚
# è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒå’Œå¯»æ‰¾æ–°çš„åµŒå…¥æ¥è¡¨ç¤ºæ‚¨æä¾›çš„å›¾åƒï¼Œå¹¶åœ¨æç¤ºä¸­ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šè¯æ±‡ã€‚å› æ­¤ï¼Œæ‰©æ•£æ¨¡å‹çš„æƒé‡ä¿æŒä¸å˜ï¼Œè®­ç»ƒè¿‡ç¨‹äº§ç”Ÿä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„ï¼ˆå‡  KBï¼‰æ–‡ä»¶ã€‚

# å› ä¸ºæ–‡æœ¬åè½¬åˆ›å»ºåµŒå…¥ï¼Œå®ƒä¸èƒ½å•ç‹¬ä½¿ç”¨ï¼Œå¦‚ DreamBoothï¼Œéœ€è¦å¦ä¸€ä¸ªæ¨¡å‹ã€‚
pipeline = AutoPipelineForText2Image.from_pretrained(
    "stable-diffusion-v1-5/stable-diffusion-v1-5", torch_dtype=torch.float16
).to("cuda")
pipeline.load_textual_inversion("sd-concepts-library/gta5-artwork")
prompt = "A detailed description of the desired image"
image = pipeline(prompt).images[0]
# æ–‡æœ¬åè½¬ä¹Ÿå¯ä»¥ç”¨äºè®­ç»ƒä¸å¸Œæœ›å‡ºç°çš„äº‹ç‰©ï¼Œä»¥åˆ›å»ºè´Ÿé¢åµŒå…¥ï¼Œä»è€Œé˜»æ­¢æ¨¡å‹ç”ŸæˆåŒ…å«è¿™äº›ä¸å¸Œæœ›å‡ºç°çš„äº‹ç‰©ï¼ˆå¦‚æ¨¡ç³Šçš„å›¾åƒæˆ–æ‰‹ä¸Šçš„å¤šä½™æ‰‹æŒ‡ï¼‰çš„å›¾åƒã€‚
# è¿™å¯ä»¥æ˜¯ä¸€ç§å¿«é€Ÿæé«˜ä½ çš„æç¤ºçš„æ–¹æ³•ã€‚ä½ è¿˜å°†ä½¿ç”¨ load_textual_inversion() å‡½æ•°åŠ è½½åµŒå…¥ï¼Œä½†è¿™æ¬¡éœ€è¦ä¸¤ä¸ªé¢å¤–çš„å‚æ•°
# weight_name:æŒ‡å®šåŠ è½½çš„æƒé‡æ–‡ä»¶ï¼Œå¦‚æœæ–‡ä»¶ä»¥ç‰¹å®šåç§°ä¿å­˜åœ¨ ğŸ¤— Diffusers æ ¼å¼æˆ–å­˜å‚¨åœ¨ A1111 æ ¼å¼ä¸­
# token: æŒ‡å®šåœ¨æç¤ºä¸­ä½¿ç”¨çš„ç‰¹æ®Šè¯ä»¥è§¦å‘åµŒå…¥

prompt = "A cute cat with three eyes"
negative_prompt = "A cute cat with four eyes"
image = pipeline(
    prompt, negative_prompt=negative_prompt, num_inference_steps=50
).images[0]

# LoRA æ˜¯ä¸€ç§æµè¡Œçš„è®­ç»ƒæŠ€æœ¯ï¼Œå› ä¸ºå®ƒé€Ÿåº¦å¿«ä¸”ç”Ÿæˆçš„æ–‡ä»¶å¤§å°æ›´å°ï¼ˆçº¦å‡ ç™¾ MBï¼‰ã€‚
# ä¸LoRA å¯ä»¥ä½¿æ¨¡å‹ä»…ä»å°‘é‡å›¾åƒä¸­å­¦ä¹ æ–°é£æ ¼ã€‚å®ƒé€šè¿‡å‘æ‰©æ•£æ¨¡å‹ä¸­æ’å…¥æ–°æƒé‡æ¥å®ç°ï¼Œç„¶ååªè®­ç»ƒæ–°æƒé‡è€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹ã€‚è¿™ä½¿å¾— LoRA çš„è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œå­˜å‚¨æ›´å®¹æ˜“
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16
).to("cuda")
pipeline.load_lora_weights(
    "ostris/super-cereal-sdxl-lora", weight_name="cereal_box_sdxl_v1.safetensors"
)
prompt = "bears, pizza bites"
image = pipeline(prompt).images[0]


# The load_lora_weights() æ–¹æ³•å°† LoRA æƒé‡åŠ è½½åˆ° UNet å’Œæ–‡æœ¬ç¼–ç å™¨ä¸­ã€‚
# è¿™æ˜¯åŠ è½½ LoRAs çš„é¦–é€‰æ–¹æ³•ï¼Œå› ä¸ºå®ƒå¯ä»¥å¤„ç†ä»¥ä¸‹æƒ…å†µï¼š
# LoRA æƒé‡æ²¡æœ‰ä¸º UNet å’Œæ–‡æœ¬ç¼–ç å™¨åˆ†åˆ«è®¾ç½®æ ‡è¯†ç¬¦
# LoRA æƒé‡ä¸º UNet å’Œæ–‡æœ¬ç¼–ç å™¨åˆ†åˆ«è®¾ç½®äº†æ ‡è¯†ç¬¦

# ä½¿ç”¨ weight_name å‚æ•°æŒ‡å®šç‰¹å®šçš„æƒé‡æ–‡ä»¶ï¼Œå¹¶ä½¿ç”¨ prefix å‚æ•°ç­›é€‰é€‚å½“çš„ state dictsï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ä¸º "unet" ï¼‰ä»¥åŠ è½½ã€‚

from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16
).to("cuda")

pipeline.unet.load_lora_adapter(
    "jbilcke-hf/sdxl-cinematic-1",
    weight_name="pytorch_lora_weights.safetensors",
    prefix="unet",
)

#  use cnmt in the prompt to trigger the LoRA
prompt = "A cute cnmt eating a slice"
image = pipeline(prompt).images[0]

# å¯¹äº load_lora_weights() å’Œ load_attn_procs()ï¼Œ
# å¯ä»¥é€šè¿‡ cross_attention_kwargs={"scale": 0.5} å‚æ•°æ¥è°ƒæ•´ä½¿ç”¨å¤šå°‘ LoRA æƒé‡ã€‚ 0 çš„å€¼ç­‰åŒäºä»…ä½¿ç”¨åŸºç¡€æ¨¡å‹æƒé‡ï¼Œè€Œ 1 çš„å€¼ç›¸å½“äºä½¿ç”¨å®Œå…¨å¾®è°ƒçš„ LoRAã€‚


# IP-é€‚é…å™¨æ˜¯ä¸€ä¸ªè½»é‡çº§çš„é€‚é…å™¨ï¼Œå®ƒä½¿ä»»ä½•æ‰©æ•£æ¨¡å‹éƒ½èƒ½è¿›è¡Œå›¾åƒæç¤ºã€‚
# è¯¥é€‚é…å™¨é€šè¿‡è§£è€¦å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„äº¤å‰æ³¨æ„åŠ›å±‚æ¥å·¥ä½œã€‚æ‰€æœ‰å…¶ä»–æ¨¡å‹ç»„ä»¶éƒ½è¢«å†»ç»“ï¼Œåªæœ‰ UNet ä¸­çš„åµŒå…¥å›¾åƒç‰¹å¾è¢«è®­ç»ƒã€‚å› æ­¤ï¼ŒIP-é€‚é…å™¨æ–‡ä»¶é€šå¸¸åªæœ‰çº¦ 100MBã€‚
from diffusers import AutoPipelineForText2Image
import torch
from diffusers import load_image

pipeline = AutoPipelineForText2Image.from_pretrained(
    "stable-diffusion-v1-5/stable-diffusion-v1-5", torch_dtype=torch.float16
).to("cuda")
pipeline.load_ip_adapter(
    "h94/IP-Adapter", subfolder="models", weight_name="ip-adapter_sd15.bin"
)
image = load_image(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/load_neg_embed.png"
)
images = pipeline(
    prompt="best quality, high quality",
    ip_adapter_image=image,
    negative_prompt="worst quality, low quality",
    num_inference_steps=50,
).images[0]
from transformers import CLIPVisionModelWithProjection

image_encoder = CLIPVisionModelWithProjection.from_pretrained(
    "h94/IP-Adapter", subfolder="models/image_encoder", torch_dtype=torch.float16
)

pipeline = AutoPipelineForText2Image.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16,
    image_encoder=image_encoder,
).to("cuda")
pipeline.load_ip_adapter(
    "h94/IP-Adapter",
    subfolder="sdxl_models",
    weight_name="ip-adapter-plus_sdxl_vit-h.safetensors",
)

# CnotrolNet
# æ§åˆ¶ç½‘æ¨¡å‹æ˜¯è¾…åŠ©æ¨¡å‹æˆ–é€‚é…å™¨ï¼Œå®ƒä»¬åœ¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼ˆå¦‚ Stable Diffusion v1.5ï¼‰ä¹‹ä¸Šè¿›è¡Œå¾®è°ƒã€‚ä½¿ç”¨æ§åˆ¶ç½‘æ¨¡å‹ä¸æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç»“åˆï¼Œæä¾›äº†æ›´å¤šæ˜¾å¼æ§åˆ¶å›¾åƒç”Ÿæˆæ–¹å¼çš„é€‰æ‹©ã€‚ä½¿ç”¨æ§åˆ¶ç½‘æ—¶ï¼Œæ‚¨å‘æ¨¡å‹æ·»åŠ ä¸€ä¸ªé¢å¤–çš„æ¡ä»¶è¾“å…¥å›¾åƒã€‚
# ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æä¾›ä¸€ä¸ªè¡¨ç¤ºäººç±»å§¿æ€çš„å›¾åƒï¼ˆé€šå¸¸è¡¨ç¤ºä¸ºå¤šä¸ªè¿æ¥æˆéª¨éª¼çš„å…³é”®ç‚¹ï¼‰ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œæ¨¡å‹å°†ç”Ÿæˆéµå¾ªè¯¥å§¿æ€çš„å›¾åƒã€‚æŸ¥çœ‹æ›´æ·±å…¥çš„æ§åˆ¶ç½‘æŒ‡å—ï¼Œäº†è§£å…¶ä»–æ¡ä»¶è¾“å…¥åŠå…¶ä½¿ç”¨æ–¹æ³•

from diffusers import ControlNetModel, AutoPipelineForText2Image
from diffusers.utils import load_image

controlnet = ControlNetModel.from_pretrained(
    "llyasviel/control_v11p_sd15_openpose", torch_dtype=torch.float16, variant="fp16"
).to("cuda")

pose_image = load_image(
    "https://huggingface.co/lllyasviel/control_v11p_sd15_openpose/resolve/main/images/control.png"
)
pipeline = AutoPipelineForText2Image.from_pretrained(
    "stable-diffusion-v1-5/stable-diffusion-v1-5",
    controlnet=controlnet,
    torch_dtype=torch.float16,
    variant="fp16",
).to("cuda")

# guidance scale
# guidance_scale å‚æ•°å½±å“æç¤ºå¯¹å›¾åƒç”Ÿæˆçš„å½±å“ç¨‹åº¦ã€‚
# è¾ƒä½çš„å€¼èµ‹äºˆæ¨¡å‹â€œåˆ›é€ åŠ›â€ï¼Œç”Ÿæˆä¸æç¤ºæ›´æ¾æ•£ç›¸å…³çš„å›¾åƒã€‚è¾ƒé«˜çš„ guidance_scale å€¼æ¨åŠ¨æ¨¡å‹æ›´ç´§å¯†åœ°éµå¾ªæç¤ºï¼Œå¦‚æœè¿™ä¸ªå€¼å¤ªé«˜ï¼Œä½ å¯èƒ½ä¼šåœ¨ç”Ÿæˆçš„å›¾åƒä¸­è§‚å¯Ÿåˆ°ä¸€äº›ä¼ªå½±ã€‚
